{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><h1> Optimizacion </h1></center>\n",
    "<center><h2>Gradiente descendente</h2></center>\n",
    "\n",
    "En este laboratoria se implementara el gradiente descendiente:\n",
    "\n",
    "1. El metodo Steepest Decent con las condiciones de Armijo and Wolfe<br>\n",
    "<br>\n",
    "2. Metodod de Newton"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Usaremos el paquete [autograd](https://github.com/HIPS/autograd) para calcular el gradiente y la Hessiana de nuestra funcion objetivo. <i>autograd</i> usa diferenciacion automatica. Basado en el lab del profesor "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "#Use np.linalg.solve to solve A^-1*b\n",
    "\n",
    "# to calculate the gradient and Hessian of the objective function\n",
    "from autograd import grad\n",
    "from autograd import hessian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective_function(x): \n",
    "    f = 0.5 * (x[0] ** 2 + 20 * x[1] ** 2)\n",
    "    return f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rosenbrock(x):\n",
    "    return ((x[0]-1)**2 + 100*(x[1]-x[0]**2)**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def polynomial(x): \n",
    "    return - 1.0 / 6 * x[0]**6 + 1 / 4 *x[0]**4 + 2 * x[0]**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_function(func, x): \n",
    "    return grad(func)(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hessian_function(func,x):\n",
    "    return hessian(func)(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input: (objective function, initial guess, step rule, c1, c2, initial alpha, optimality tolerance)\n",
    "def steepest_descent(function, x0, stepRule, c1, c2, alpha0, tol=0.01):     \n",
    "    \n",
    "    xCur = x0\n",
    "    fcur = function(xCur)\n",
    "    gCur = gradient_function(function,xCur)\n",
    "    \n",
    "    # norm of the gradient at initial point for optimality condition\n",
    "    nmg0 = np.linalg.norm(gradient_function(function, x0))\n",
    "    \n",
    "    # count number of steps taken by method\n",
    "    it = 0\n",
    "    \n",
    "    # accumulate number of iterations needed by linesearch algorithm in every step\n",
    "    ls_iter = 0\n",
    "    \n",
    "    # store iterates for plotting\n",
    "    xIter=[]\n",
    "    xIter.append(x0)\n",
    "    \n",
    "    while(np.linalg.norm(gCur)>tol*min(1,nmg0)): \n",
    "\n",
    "        it=it+1\n",
    "        \n",
    "        # calculate descent direction\n",
    "        direction = # add your code here\n",
    "        \n",
    "        # calculate step-length\n",
    "        if (stepRule=='armijo'):\n",
    "            alpha, ls_ = armijo(function,xCur,direction, c1, alpha0)\n",
    "        elif (stepRule=='wolfe'):\n",
    "            alpha, ls_ = wolfe(function,xCur,direction, c1, c2, alpha0)\n",
    "        else:\n",
    "            alpha = 0.01\n",
    "        ls_iter = ls_iter + ls_\n",
    "        \n",
    "        # update current point\n",
    "        \n",
    "        # add your code here\n",
    "        \n",
    "        xIter.append(xCur)\n",
    "\n",
    "    return xCur, fcur, it, xIter, ls_iter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input: (objective function, initial guess, optimality tolerance)\n",
    "def newton_descent(function, x0, stepRule='no', tol=0.01): \n",
    "\n",
    "    xCur = x0\n",
    "    fcur = function(xCur)\n",
    "    gCur = gradient_function(function,xCur)\n",
    "    hCur = hessian_function(function,xCur)    \n",
    "    \n",
    "    # norm of the gradient at initial point for optimality condition\n",
    "    nmg0 = np.linalg.norm(gradient_function(function, x0))    \n",
    "    \n",
    "    # count number of steps taken by method    \n",
    "    it = 0\n",
    "    \n",
    "    # store iterates for plotting    \n",
    "    xIter=[]\n",
    "    xIter.append(x0)\n",
    "    \n",
    "    while(np.linalg.norm(gCur)>tol*min(1,nmg0)):\n",
    "        \n",
    "        it=it+1\n",
    "        \n",
    "        # calculate descent direction\n",
    "        direction = # add your code here\n",
    "        \n",
    "        # calculate step-length or use natural length of 1\n",
    "        if (stepRule ==  # add your code here\n",
    "            alpha = # add your code here\n",
    "        \n",
    "        # update current point\n",
    "\n",
    "        # add your code here\n",
    "        \n",
    "        xIter.append(xCur)\n",
    "\n",
    "    return xCur, fcur, it, xIter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def armijo(function, xcur, searchdirection, c1, alpha0): \n",
    "    #f(x+alpha p) \\leq f(x) + c1 alpha grad(f).p\n",
    "    \n",
    "    alpha = alpha0\n",
    "    xnew = xcur + alpha * searchdirection\n",
    "    fcur = function(xcur)\n",
    "    fnew = function(xnew)\n",
    "    gradientCur = gradient_function(function,xcur)\n",
    "    numiter = 0\n",
    "    # check for Armijo condition\n",
    "    while (# add your code here): \n",
    "        numiter += 1\n",
    "        alpha = alpha/2.0\n",
    "        xnew = xcur + alpha * searchdirection\n",
    "        fnew = function(xnew)\n",
    "\n",
    "    return alpha, numiter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wolfe(function, xcur, searchdirection, c1, c2, alpha0): \n",
    "    #f(x+alpha p) \\leq f(x) + c1 alpha grad(f)Tp\n",
    "    #grad(f(x+alpha p))Tp \\geq c2 grad(f)Tp\n",
    "    \n",
    "    alpha = alpha0\n",
    "    xnew = xcur + alpha * searchdirection\n",
    "    fcur = function(xcur)\n",
    "    fnew = function(xnew)\n",
    "    gradientCur = gradient_function(function,xcur)\n",
    "    gradientNew = gradient_function(function,xnew)\n",
    "    numiter = 0\n",
    "    \n",
    "    lb = 0\n",
    "    ub = np.inf \n",
    "    \n",
    "    # check for Wolfe conditions\n",
    "    # adapted from https://sites.math.washington.edu/~burke/crs/408/notes/nlp/line.pdf (pg. 8)\n",
    "    while True: \n",
    "        numiter += 1\n",
    "        if ((fnew) > (fcur + c1 * alpha * np.dot(gradientCur,searchdirection))):\n",
    "            ub = alpha\n",
    "            alpha = 0.5 * (lb + ub)\n",
    "        elif (np.dot(gradientNew,searchdirection) < c2 * np.dot(gradientCur,searchdirection)):\n",
    "            lb = alpha\n",
    "            if np.isinf(ub):\n",
    "                alpha = 2 * lb\n",
    "            else:\n",
    "                alpha = 0.5 * (lb + ub)\n",
    "        else:\n",
    "            break\n",
    "        xnew = xcur + alpha * searchdirection\n",
    "        fnew = function(xnew)\n",
    "        gradientNew = gradient_function(function,xnew)\n",
    "\n",
    "    return alpha, numiter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solving quadratic unconstrained problem\n",
    "###  Steepest descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x0 = np.array([0.9, 0.1])\n",
    "c1 = 0.5 \n",
    "c2 = 0.6 \n",
    "alpha0 = 0.125 \n",
    "tol = 0.0001\n",
    "\n",
    "print (\"{:<40} {:^20} {:^20} {:^15} {:^15}\".format('Method','xopt','fopt','iter','ls_iter'))\n",
    "\n",
    "# using steepest descent with Armijo condition for linesearch\n",
    "xopt, fopt, it, xIter, ls_iter = steepest_descent(objective_function, x0, 'armijo', c1, c2, alpha0)\n",
    "print (\"{:<40} [{:^6.4f}, {:^6.4f}] {:<8} {:<20.4e} {:<15d} {:<15d}\".format('Armijo with default tol = 0.01',xopt[0],xopt[1],' ',fopt,it,ls_iter))\n",
    "\n",
    "print(\"\\n\")\n",
    "\n",
    "xopt, fopt, it, xIter, ls_iter = steepest_descent(objective_function, x0, 'wolfe', c1, c2, alpha0)\n",
    "print (\"{:<40} [{:^6.4f}, {:^6.4f}] {:<8} {:<20.4e} {:<15d} {:<15d}\".format('Wolfe with default tol = 0.01',xopt[0],xopt[1],' ',fopt,it,ls_iter))\n",
    "\n",
    "print(\"\\n\")\n",
    "\n",
    "xopt, fopt, it, xIter, ls_iter = steepest_descent(objective_function, x0, 'armijo', c1, c2, alpha0, tol)\n",
    "print (\"{:<40} [{:^6.4f}, {:^6.4f}] {:<8} {:<20.4e} {:<15d} {:<15d}\".format('Armijo with tol = 0.0001',xopt[0],xopt[1],' ',fopt,it,ls_iter))\n",
    "\n",
    "print(\"\\n\")\n",
    "\n",
    "\n",
    "xopt, fopt, it, xIter, ls_iter = steepest_descent(objective_function, x0, 'wolfe', c1, c2, alpha0, tol)\n",
    "print (\"{:<40} [{:^6.4f}, {:^6.4f}] {:<8} {:<20.4e} {:<15d} {:<15d}\".format('Wolfe with tol = 0.0001',xopt[0],xopt[1],' ',fopt,it,ls_iter))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Newton's method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (\"{:<40} {:^20} {:^20} {:^15}\".format('Method','xopt','fopt','iter'))\n",
    "\n",
    "xopt, fopt, it, xIter = newton_descent(objective_function, x0)\n",
    "print (\"{:<40} [{:^6.4f}, {:^6.4f}] {:<8} {:<20.4e} {:<15d}\".format('Newton with default tol = 0.01',xopt[0],xopt[1],' ',fopt,it))\n",
    "print(\"\\n\")\n",
    "\n",
    "xopt, fopt, it, xIter = newton_descent(objective_function, x0, tol=0.0001)\n",
    "print (\"{:<40} [{:^6.4f}, {:^6.4f}] {:<8} {:<20.4e} {:<15d}\".format('Newton with tol = 0.0001',xopt[0],xopt[1],' ',fopt,it))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solving Rosenbrock's function\n",
    "Try also different starting points and play around with the tuning factors.\n",
    "\n",
    "### Steepest descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "x0 = np.array([0.9, 0.1])\n",
    "c1 = 0.5 \n",
    "c2 = 0.6 \n",
    "alpha0 = 0.125 \n",
    "tol = 0.0001\n",
    "\n",
    "print (\"{:<40} {:^20} {:^20} {:^15} {:^15}\".format('Method','xopt','fopt','iter','ls_iter'))\n",
    "\n",
    "# using steepest descent with Armijo condition for linesearch\n",
    "xopt, fopt, it, xIter, ls_iter = steepest_descent(rosenbrock, x0, 'armijo', c1, c2, alpha0)\n",
    "print (\"{:<40} [{:^6.4f}, {:^6.4f}] {:<8} {:<20.4e} {:<15d} {:<15d}\".format('Armijo with default tol = 0.01',xopt[0],xopt[1],' ',fopt,it,ls_iter))\n",
    "\n",
    "print(\"\\n\")\n",
    "\n",
    "xopt, fopt, it, xIter, ls_iter = steepest_descent(rosenbrock, x0, 'wolfe', c1, c2, alpha0)\n",
    "print (\"{:<40} [{:^6.4f}, {:^6.4f}] {:<8} {:<20.4e} {:<15d} {:<15d}\".format('Wolfe with default tol = 0.01',xopt[0],xopt[1],' ',fopt,it,ls_iter))\n",
    "\n",
    "print(\"\\n\")\n",
    "\n",
    "xopt, fopt, it, xIter, ls_iter = steepest_descent(rosenbrock, x0, 'armijo', c1, c2, alpha0, tol)\n",
    "print (\"{:<40} [{:^6.4f}, {:^6.4f}] {:<8} {:<20.4e} {:<15d} {:<15d}\".format('Armijo with tol = 0.0001',xopt[0],xopt[1],' ',fopt,it,ls_iter))\n",
    "\n",
    "print(\"\\n\")\n",
    "\n",
    "\n",
    "xopt, fopt, it, xIter, ls_iter = steepest_descent(rosenbrock, x0, 'wolfe', c1, c2, alpha0, tol)\n",
    "print (\"{:<40} [{:^6.4f}, {:^6.4f}] {:<8} {:<20.4e} {:<15d} {:<15d}\".format('Wolfe with tol = 0.0001',xopt[0],xopt[1],' ',fopt,it,ls_iter))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Newton's method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print (\"{:<40} {:^20} {:^20} {:^15}\".format('Method','xopt','fopt','iter'))\n",
    "\n",
    "xopt, fopt, it, xIter = newton_descent(rosenbrock, x0)\n",
    "print (\"{:<40} [{:^6.4f}, {:^6.4f}] {:<8} {:<20.4e} {:<15d}\".format('Newton with default tol = 0.01',xopt[0],xopt[1],' ',fopt,it))\n",
    "print(\"\\n\")\n",
    "\n",
    "xopt, fopt, it, xIter = newton_descent(rosenbrock, x0, tol=0.0001)\n",
    "print (\"{:<40} [{:^6.4f}, {:^6.4f}] {:<8} {:<20.4e} {:<15d}\".format('Newton with tol = 0.0001',xopt[0],xopt[1],' ',fopt,it))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple polynomial function\n",
    "Here, a simple polynomial function is considered. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x0 = np.array([1.0])\n",
    "\n",
    "xopt, fopt, it, xIter, ls_iter = steepest_descent(polynomial, x0, 'armijo', c1, c2, alpha0)\n",
    "print (\"{:<40} [{:^6.4f}] {:<8} {:<20.4e} {:<15d}\".format('Steepest descent with default tol = 0.01',xopt[0],' ',fopt,it))\n",
    "\n",
    "xopt, fopt, it, xIter = newton_descent(polynomial, x0, stepRule='armijo')\n",
    "print (\"{:<40} [{:^6.4f}] {:<8} {:<20.4e} {:<15d}\".format('Newton with Armijo default tol = 0.01',xopt[0],' ',fopt,it))\n",
    "\n",
    "xopt, fopt, it, xIter = newton_descent(polynomial, x0)\n",
    "print (\"{:<40} [{:^6.4f}] {:<8} {:<20.4e} {:<15d}\".format('Newton with default tol = 0.01',xopt[0],' ',fopt,it))\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
