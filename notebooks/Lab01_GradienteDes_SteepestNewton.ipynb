{"cells":[{"cell_type":"markdown","metadata":{"id":"xOAM9kkLFtei"},"source":["<center><h1> Optimizacion </h1></center>\n","<center><h2>Gradiente descendente</h2></center>\n","\n","En este laboratoria se implementara el gradiente descendiente:\n","\n","1. El metodo Steepest Decent con las condiciones de Armijo and Wolfe<br>\n","<br>\n","2. Metodod de Newton"]},{"cell_type":"markdown","metadata":{"id":"LjI6ZWMaFtel"},"source":["Usaremos el paquete [autograd](https://github.com/HIPS/autograd) para calcular el gradiente y la Hessiana de nuestra funcion objetivo. <i>autograd</i> usa diferenciacion automatica. Basado en el lab del profesor "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"mytr1j8kFtem"},"outputs":[],"source":["import numpy as np\n","\n","#Use np.linalg.solve to solve A^-1*b\n","\n","# to calculate the gradient and Hessian of the objective function\n","from autograd import grad\n","from autograd import hessian"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"aBbKyu_xFten"},"outputs":[],"source":["def objective_function(x): \n","    f = 0.5 * (x[0] ** 2 + 20 * x[1] ** 2)\n","    return f"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"BQvpPinfFteo"},"outputs":[],"source":["def rosenbrock(x):\n","    return ((x[0]-1)**2 + 100*(x[1]-x[0]**2)**2)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"mcRUMFYfFteo"},"outputs":[],"source":["def polynomial(x): \n","    return - 1.0 / 6 * x[0]**6 + 1 / 4 *x[0]**4 + 2 * x[0]**2"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"NHmRmF2_Fteo"},"outputs":[],"source":["def gradient_function(func, x): \n","    return grad(func)(x)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"pkSNN-DyFtep"},"outputs":[],"source":["def hessian_function(func,x):\n","    return hessian(func)(x)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3hTiptt1Ftep"},"outputs":[],"source":["# input: (objective function, initial guess, step rule, c1, c2, initial alpha, optimality tolerance)\n","def steepest_descent(function, x0, stepRule, c1, c2, alpha0, tol=0.01):     \n","    \n","    xCur = x0\n","    fcur = function(xCur)\n","    gCur = gradient_function(function,xCur)\n","    \n","    # norm of the gradient at initial point for optimality condition\n","    nmg0 = np.linalg.norm(gradient_function(function, x0))\n","    \n","    # count number of steps taken by method\n","    it = 0\n","    \n","    # accumulate number of iterations needed by linesearch algorithm in every step\n","    ls_iter = 0\n","    \n","    # store iterates for plotting\n","    xIter=[]\n","    xIter.append(x0)\n","    \n","    while(np.linalg.norm(gCur)>tol*min(1,nmg0)): \n","\n","        it=it+1\n","        \n","        # calculate descent direction\n","        direction = # add your code here\n","        \n","        # calculate step-length\n","        if (stepRule=='armijo'):\n","            alpha, ls_ = armijo(function,xCur,direction, c1, alpha0)\n","        elif (stepRule=='wolfe'):\n","            alpha, ls_ = wolfe(function,xCur,direction, c1, c2, alpha0)\n","        else:\n","            alpha = 0.01\n","        ls_iter = ls_iter + ls_\n","        \n","        # update current point\n","        \n","        # add your code here\n","        \n","        xIter.append(xCur)\n","\n","    return xCur, fcur, it, xIter, ls_iter"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"rQrMwWIdFteq"},"outputs":[],"source":["# input: (objective function, initial guess, optimality tolerance)\n","def newton_descent(function, x0, stepRule='no', tol=0.01): \n","\n","    xCur = x0\n","    fcur = function(xCur)\n","    gCur = gradient_function(function,xCur)\n","    hCur = hessian_function(function,xCur)    \n","    \n","    # norm of the gradient at initial point for optimality condition\n","    nmg0 = np.linalg.norm(gradient_function(function, x0))    \n","    \n","    # count number of steps taken by method    \n","    it = 0\n","    \n","    # store iterates for plotting    \n","    xIter=[]\n","    xIter.append(x0)\n","    \n","    while(np.linalg.norm(gCur)>tol*min(1,nmg0)):\n","        \n","        it=it+1\n","        \n","        # calculate descent direction\n","        direction = # add your code here\n","        \n","        # calculate step-length or use natural length of 1\n","        if (stepRule ==  # add your code here\n","            alpha = # add your code here\n","        \n","        # update current point\n","\n","        # add your code here\n","        \n","        xIter.append(xCur)\n","\n","    return xCur, fcur, it, xIter"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"dsLqasITFter"},"outputs":[],"source":["def armijo(function, xcur, searchdirection, c1, alpha0): \n","    #f(x+alpha p) \\leq f(x) + c1 alpha grad(f).p\n","    \n","    alpha = alpha0\n","    xnew = xcur + alpha * searchdirection\n","    fcur = function(xcur)\n","    fnew = function(xnew)\n","    gradientCur = gradient_function(function,xcur)\n","    numiter = 0\n","    # check for Armijo condition\n","    while (# add your code here): \n","        numiter += 1\n","        alpha = alpha/2.0\n","        xnew = xcur + alpha * searchdirection\n","        fnew = function(xnew)\n","\n","    return alpha, numiter"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"BND3Qzf1Fter"},"outputs":[],"source":["def wolfe(function, xcur, searchdirection, c1, c2, alpha0): \n","    #f(x+alpha p) \\leq f(x) + c1 alpha grad(f)Tp\n","    #grad(f(x+alpha p))Tp \\geq c2 grad(f)Tp\n","    \n","    alpha = alpha0\n","    xnew = xcur + alpha * searchdirection\n","    fcur = function(xcur)\n","    fnew = function(xnew)\n","    gradientCur = gradient_function(function,xcur)\n","    gradientNew = gradient_function(function,xnew)\n","    numiter = 0\n","    \n","    lb = 0\n","    ub = np.inf \n","    \n","    # check for Wolfe conditions\n","    # adapted from https://sites.math.washington.edu/~burke/crs/408/notes/nlp/line.pdf (pg. 8)\n","    while True: \n","        numiter += 1\n","        if ((fnew) > (fcur + c1 * alpha * np.dot(gradientCur,searchdirection))):\n","            ub = alpha\n","            alpha = 0.5 * (lb + ub)\n","        elif (np.dot(gradientNew,searchdirection) < c2 * np.dot(gradientCur,searchdirection)):\n","            lb = alpha\n","            if np.isinf(ub):\n","                alpha = 2 * lb\n","            else:\n","                alpha = 0.5 * (lb + ub)\n","        else:\n","            break\n","        xnew = xcur + alpha * searchdirection\n","        fnew = function(xnew)\n","        gradientNew = gradient_function(function,xnew)\n","\n","    return alpha, numiter"]},{"cell_type":"markdown","metadata":{"id":"PfYIkH1WFtes"},"source":["## Solving quadratic unconstrained problem\n","###  Steepest descent"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ZknNsC1tFtes"},"outputs":[],"source":["x0 = np.array([0.9, 0.1])\n","c1 = 0.5 \n","c2 = 0.6 \n","alpha0 = 0.125 \n","tol = 0.0001\n","\n","print (\"{:<40} {:^20} {:^20} {:^15} {:^15}\".format('Method','xopt','fopt','iter','ls_iter'))\n","\n","# using steepest descent with Armijo condition for linesearch\n","xopt, fopt, it, xIter, ls_iter = steepest_descent(objective_function, x0, 'armijo', c1, c2, alpha0)\n","print (\"{:<40} [{:^6.4f}, {:^6.4f}] {:<8} {:<20.4e} {:<15d} {:<15d}\".format('Armijo with default tol = 0.01',xopt[0],xopt[1],' ',fopt,it,ls_iter))\n","\n","print(\"\\n\")\n","\n","xopt, fopt, it, xIter, ls_iter = steepest_descent(objective_function, x0, 'wolfe', c1, c2, alpha0)\n","print (\"{:<40} [{:^6.4f}, {:^6.4f}] {:<8} {:<20.4e} {:<15d} {:<15d}\".format('Wolfe with default tol = 0.01',xopt[0],xopt[1],' ',fopt,it,ls_iter))\n","\n","print(\"\\n\")\n","\n","xopt, fopt, it, xIter, ls_iter = steepest_descent(objective_function, x0, 'armijo', c1, c2, alpha0, tol)\n","print (\"{:<40} [{:^6.4f}, {:^6.4f}] {:<8} {:<20.4e} {:<15d} {:<15d}\".format('Armijo with tol = 0.0001',xopt[0],xopt[1],' ',fopt,it,ls_iter))\n","\n","print(\"\\n\")\n","\n","\n","xopt, fopt, it, xIter, ls_iter = steepest_descent(objective_function, x0, 'wolfe', c1, c2, alpha0, tol)\n","print (\"{:<40} [{:^6.4f}, {:^6.4f}] {:<8} {:<20.4e} {:<15d} {:<15d}\".format('Wolfe with tol = 0.0001',xopt[0],xopt[1],' ',fopt,it,ls_iter))"]},{"cell_type":"markdown","metadata":{"id":"6EYhUUIQFtes"},"source":["### Newton's method"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Rs-6occcFtet"},"outputs":[],"source":["print (\"{:<40} {:^20} {:^20} {:^15}\".format('Method','xopt','fopt','iter'))\n","\n","xopt, fopt, it, xIter = newton_descent(objective_function, x0)\n","print (\"{:<40} [{:^6.4f}, {:^6.4f}] {:<8} {:<20.4e} {:<15d}\".format('Newton with default tol = 0.01',xopt[0],xopt[1],' ',fopt,it))\n","print(\"\\n\")\n","\n","xopt, fopt, it, xIter = newton_descent(objective_function, x0, tol=0.0001)\n","print (\"{:<40} [{:^6.4f}, {:^6.4f}] {:<8} {:<20.4e} {:<15d}\".format('Newton with tol = 0.0001',xopt[0],xopt[1],' ',fopt,it))"]},{"cell_type":"markdown","metadata":{"id":"2pxBRyt1Ftet"},"source":["## Solving Rosenbrock's function\n","Try also different starting points and play around with the tuning factors.\n","\n","### Steepest descent"]},{"cell_type":"code","execution_count":null,"metadata":{"scrolled":true,"id":"26ByyQGwFtet"},"outputs":[],"source":["x0 = np.array([0.9, 0.1])\n","c1 = 0.5 \n","c2 = 0.6 \n","alpha0 = 0.125 \n","tol = 0.0001\n","\n","print (\"{:<40} {:^20} {:^20} {:^15} {:^15}\".format('Method','xopt','fopt','iter','ls_iter'))\n","\n","# using steepest descent with Armijo condition for linesearch\n","xopt, fopt, it, xIter, ls_iter = steepest_descent(rosenbrock, x0, 'armijo', c1, c2, alpha0)\n","print (\"{:<40} [{:^6.4f}, {:^6.4f}] {:<8} {:<20.4e} {:<15d} {:<15d}\".format('Armijo with default tol = 0.01',xopt[0],xopt[1],' ',fopt,it,ls_iter))\n","\n","print(\"\\n\")\n","\n","xopt, fopt, it, xIter, ls_iter = steepest_descent(rosenbrock, x0, 'wolfe', c1, c2, alpha0)\n","print (\"{:<40} [{:^6.4f}, {:^6.4f}] {:<8} {:<20.4e} {:<15d} {:<15d}\".format('Wolfe with default tol = 0.01',xopt[0],xopt[1],' ',fopt,it,ls_iter))\n","\n","print(\"\\n\")\n","\n","xopt, fopt, it, xIter, ls_iter = steepest_descent(rosenbrock, x0, 'armijo', c1, c2, alpha0, tol)\n","print (\"{:<40} [{:^6.4f}, {:^6.4f}] {:<8} {:<20.4e} {:<15d} {:<15d}\".format('Armijo with tol = 0.0001',xopt[0],xopt[1],' ',fopt,it,ls_iter))\n","\n","print(\"\\n\")\n","\n","\n","xopt, fopt, it, xIter, ls_iter = steepest_descent(rosenbrock, x0, 'wolfe', c1, c2, alpha0, tol)\n","print (\"{:<40} [{:^6.4f}, {:^6.4f}] {:<8} {:<20.4e} {:<15d} {:<15d}\".format('Wolfe with tol = 0.0001',xopt[0],xopt[1],' ',fopt,it,ls_iter))"]},{"cell_type":"markdown","metadata":{"id":"s-L0xutDFteu"},"source":["### Newton's method"]},{"cell_type":"code","execution_count":null,"metadata":{"scrolled":true,"id":"lDfPJ7IKFteu"},"outputs":[],"source":["print (\"{:<40} {:^20} {:^20} {:^15}\".format('Method','xopt','fopt','iter'))\n","\n","xopt, fopt, it, xIter = newton_descent(rosenbrock, x0)\n","print (\"{:<40} [{:^6.4f}, {:^6.4f}] {:<8} {:<20.4e} {:<15d}\".format('Newton with default tol = 0.01',xopt[0],xopt[1],' ',fopt,it))\n","print(\"\\n\")\n","\n","xopt, fopt, it, xIter = newton_descent(rosenbrock, x0, tol=0.0001)\n","print (\"{:<40} [{:^6.4f}, {:^6.4f}] {:<8} {:<20.4e} {:<15d}\".format('Newton with tol = 0.0001',xopt[0],xopt[1],' ',fopt,it))"]},{"cell_type":"markdown","metadata":{"id":"wUtctjWPFteu"},"source":["## Simple polynomial function\n","Here, a simple polynomial function is considered. "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"AC7mGGEdFtev"},"outputs":[],"source":["x0 = np.array([1.0])\n","\n","xopt, fopt, it, xIter, ls_iter = steepest_descent(polynomial, x0, 'armijo', c1, c2, alpha0)\n","print (\"{:<40} [{:^6.4f}] {:<8} {:<20.4e} {:<15d}\".format('Steepest descent with default tol = 0.01',xopt[0],' ',fopt,it))\n","\n","xopt, fopt, it, xIter = newton_descent(polynomial, x0, stepRule='armijo')\n","print (\"{:<40} [{:^6.4f}] {:<8} {:<20.4e} {:<15d}\".format('Newton with Armijo default tol = 0.01',xopt[0],' ',fopt,it))\n","\n","xopt, fopt, it, xIter = newton_descent(polynomial, x0)\n","print (\"{:<40} [{:^6.4f}] {:<8} {:<20.4e} {:<15d}\".format('Newton with default tol = 0.01',xopt[0],' ',fopt,it))\n","\n"]}],"metadata":{"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.13"},"colab":{"provenance":[{"file_id":"https://github.com/cgl-itm/Optimizacion-ITM/blob/main/notebooks/04_GradienteDes_SteepedtNewton.ipynb","timestamp":1678981406093}]}},"nbformat":4,"nbformat_minor":0}