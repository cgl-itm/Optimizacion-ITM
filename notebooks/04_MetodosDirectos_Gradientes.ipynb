{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Metodos directos - Basados en gradiente\n",
    "Para optimización de funciones matematicas suaves (diferenciables al menos 2 veces) sin restricciones. Basado en los notebooks de Kaan Öztürk disponibles en https://github.com/mkozturk/notebooks/tree/master."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pylab as plt\n",
    "import numpy as np"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consideremos la función Rosenbrock\n",
    "$$f(x,y)=10(y-x^2)^2 + (1-x)^2$$\n",
    "con gradiente\n",
    "$$\\nabla f = \\left[\\begin{array}{c}\n",
    "40x^3 - 40xy +2x - 2 \\\\\\\n",
    "20(y-x^2)\n",
    "\\end{array}\\right]$$\n",
    "y Hessiana\n",
    "$$\\nabla^2 f = \\left[\n",
    "\\begin{array}{c}\n",
    "120x^2-40y+2 & -40x \\\\\\\n",
    "-40x & 20\n",
    "\\end{array}\\right]$$\n",
    "El unico minimo se encuentra en $(x,y)=(1,1)$ donde $f(1,1)=0$. Construimos las funciones, funcion objetivo, gradiente, Hessiana."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def objfun(x,y):\n",
    "    return 10*(y-x**2)**2 + (1-x)**2\n",
    "def gradient(x,y):\n",
    "    return np.array([-40*x*y + 40*x**3 -2 + 2*x, 20*(y-x**2)])\n",
    "def hessian(x,y):\n",
    "    return np.array([[120*x*x - 40*y+2, -40*x],[-40*x, 20]])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creamos una funcion en Python que grafica el contorno de la funcion Rosembrock."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def contourplot(objfun, xmin, xmax, ymin, ymax, ncontours=50, fill=True):\n",
    "\n",
    "    x = np.linspace(xmin, xmax, 200)\n",
    "    y = np.linspace(ymin, ymax, 200)\n",
    "    X, Y = np.meshgrid(x,y)\n",
    "    Z = objfun(X,Y)\n",
    "    if fill: # grafica contorno\n",
    "        plt.contourf(X,Y,Z,ncontours); \n",
    "    else:\n",
    "        plt.contour(X,Y,Z,ncontours);\n",
    "    plt.scatter(1,1,marker=\"x\",s=50,color=\"r\");  # marcar el minimo"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aqui graficamos una figura del contorno de la funcion Rosembrock, con el minimo global marcado con una X roja."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "contourplot(objfun, -7,7, -10, 40, fill=False)\n",
    "plt.xlabel(\"x\")\n",
    "plt.ylabel(\"y\")\n",
    "plt.title(\"Contours of $f(x,y)=10(y-x^2)^2 + (1-x)^2$\");"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Steepest descent (gradiente descendente) con paso fijo"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Primero, escribimos una funcion que utiliza el metodo de Steepest descent. Empieza la solucion en un posicion inicial (`init`), se mueve a traves del opuesto al gradiente con pasos `steplength`, hasta que la diferencia del error absoluto entre valores de la funcion caen por debajo de la tolerancia (`tolerance`) o hasta que el numero de iteraciones exceda el maximo (`maxiter`).\n",
    "\n",
    "La funcion retorna un arreglo de todas las posiciones intermedias, y los valores de la funcion objetivo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def steepestdescent(objfun, gradient, init, tolerance=1e-6, maxiter=10000, steplength=0.01):\n",
    "    p = init\n",
    "    iterno=0\n",
    "    parray = [p]\n",
    "    fprev = objfun(p[0],p[1])\n",
    "    farray = [fprev]\n",
    "    while iterno < maxiter:\n",
    "        p = p - steplength*gradient(p[0],p[1])\n",
    "        fcur = objfun(p[0], p[1])\n",
    "        if np.isnan(fcur):\n",
    "            break\n",
    "        parray.append(p)\n",
    "        farray.append(fcur)\n",
    "        if abs(fcur-fprev)<tolerance:\n",
    "            break\n",
    "        fprev = fcur\n",
    "        iterno += 1\n",
    "    return np.array(parray), np.array(farray)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora veamos como el metodo steepest descent se comporta con la funcion Rosenbrock."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Caso 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "p, f = steepestdescent(objfun, gradient, init=[2,4], steplength=0.005)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Graficar la convergencia de la solucion. Izquierda: Los puntos solucion (blanco) superpuestas sobre el grafico de contorno. La estrella indica el punto inicial. Derecha: La funcion objetivo en cada iteración."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(17,5))\n",
    "plt.subplot(1,2,1)\n",
    "contourplot(objfun, -1,3,0,10)\n",
    "plt.xlabel(\"x\")\n",
    "plt.ylabel(\"y\")\n",
    "plt.title(\"Minimize $f(x,y)=10(y-x^2)^2 + (1-x)^2$\");\n",
    "plt.scatter(p[0,0],p[0,1],marker=\"*\",color=\"w\")\n",
    "for i in range(1,len(p)):    \n",
    "        plt.plot( (p[i-1,0],p[i,0]), (p[i-1,1],p[i,1]) , \"w\");\n",
    "\n",
    "plt.subplot(1,2,2)\n",
    "plt.plot(f)\n",
    "plt.xlabel(\"iteraciones\")\n",
    "plt.ylabel(\"$f(x,y)$\");"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La solucion es muy suave. Supongamos que se incrementa el tamaño del paso desde $\\alpha=0.005$ a $\\alpha=0.01$, y la trayectoria de la solucion se torna extraña."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "p, f = steepestdescent(objfun, gradient, init=[2,4], steplength=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(17,5))\n",
    "plt.subplot(1,2,1)\n",
    "contourplot(objfun, -2,3,0,10)\n",
    "plt.xlabel(\"x\")\n",
    "plt.ylabel(\"y\")\n",
    "plt.title(\"Minimize $f(x,y)=10(y-x^2)^2 + (1-x)^2$\");\n",
    "plt.scatter(p[0,0],p[0,1],marker=\"*\",color=\"w\")\n",
    "for i in range(1,len(p)):    \n",
    "        plt.plot( (p[i-1,0],p[i,0]), (p[i-1,1],p[i,1]) , \"w\");\n",
    "\n",
    "plt.subplot(1,2,2)\n",
    "plt.plot(f)\n",
    "plt.xlabel(\"iteraciones\")\n",
    "plt.ylabel(\"$f(x,y)$\");"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora el tamaño del paso es mas largo, entonces la nueva posicion termina en una ubicacion donde el gradiente es mayor. Entonces, el siguiente paso es mas largo, y observamos un salto mas largo a traves de la montaña en la mitad.\n",
    "Desde ahi el paso se vuelve mas pequeño otra vez, y la solucion se acerca al minimo global.\n",
    "\n",
    "Intentar con valores diferentes de posicion inicial, donde el gradiente es mas grande. Ahora, el mismo $\\alpha$ es muy grande; el tamaño del paso se incrementa en cada iteracion y el calculo explota!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "p, f = steepestdescent(objfun, gradient, init=[2,6], steplength=0.01)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vemos que el valor de la funcion se incrementa rapidaente en cada iteración. El algoritmo es inestable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "f"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sin embargo, cuando $\\alpha$ se reduce a $0.005$ otra vez, vemos que la solucion converge edspues de algunas iteraciones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "p, f = steepestdescent(objfun, gradient, init=[2,6], steplength=0.005)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(17,5))\n",
    "plt.subplot(1,2,1)\n",
    "contourplot(objfun, -2,3,0,10)\n",
    "plt.xlabel(\"x\")\n",
    "plt.ylabel(\"y\")\n",
    "plt.title(\"Minimize $f(x,y)=10(y-x^2)^2 + (1-x)^2$\");\n",
    "plt.scatter(p[0,0],p[0,1],marker=\"*\",color=\"w\")\n",
    "for i in range(1,len(p)):    \n",
    "        plt.plot( (p[i-1,0],p[i,0]), (p[i-1,1],p[i,1]) , \"w\");\n",
    "\n",
    "plt.subplot(1,2,2)\n",
    "plt.plot(f)\n",
    "plt.xlim(0,100)\n",
    "plt.xlabel(\"iterations\")\n",
    "plt.ylabel(\"function value\");"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En general, la convergencia depende sensiblemente sobre el valor de $\\alpha$, como tambien del valor local del gradiente en la posicion inicial. Puede jugar con diferentes posiciones iniciales y longitudes de paso para ver como funciona.\n",
    "\n",
    "Tip: Por ensayo y error, intentar con valores de parametros que son muy cercanos a ser inestables; estos generan trayectorias locas. Por ejemplo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "p, f = steepestdescent(objfun, gradient, init=[2,5.1155], steplength=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(17,5))\n",
    "plt.subplot(1,2,1)\n",
    "contourplot(objfun, -3,3,0,10)\n",
    "plt.xlabel(\"x\")\n",
    "plt.ylabel(\"y\")\n",
    "plt.title(\"Minimize $f(x,y)=10(y-x^2)^2 + (1-x)^2$\");\n",
    "plt.scatter(p[0,0],p[0,1],marker=\"*\",color=\"w\")\n",
    "for i in range(1,len(p)):    \n",
    "        plt.plot( (p[i-1,0],p[i,0]), (p[i-1,1],p[i,1]) , \"w\");\n",
    "\n",
    "plt.subplot(1,2,2)\n",
    "plt.plot(f)\n",
    "plt.xlabel(\"iteraciones\")\n",
    "plt.ylabel(\"$f(x,y)$\");"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ejercicios\n",
    "1.  Se tiene la funcion objetivo\n",
    "    $$f(x,y) = x^2 + 4y^2 + xy,$$\n",
    "    cuyo gradiente es \n",
    "    $$\\nabla f(x,y) = (2x + y,8y + x)^T.$$\n",
    "    Esta es una función cuadrática con mínimo global en $(0,0)$, con valor $f(0,0)=0$. Encontrar el minimizador $\\mathbf{x}^*$ de la función objetivo usando gradiente descendente.\n",
    "\n",
    "2. Intentar ahora con la función $f(x,y) = |x| + |y|$, la cual no es suave. Ejecute el metodo de gradiente descendente; que observa? Optimizar tal funcion es tema de optimizacion \"no-suave\".\n",
    "3. Intente minimizar la funcion no convexa con varios minimos y puntos de ensilladura. Por ejemplo, intentar con la funcion\n",
    "$$f(x,y) = x^4 - 2x^2 + y^2.$$"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
